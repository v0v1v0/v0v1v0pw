<div class="container">

<table style="width: 100%;"><tr>
<td>ww_area_of_applicability</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Find the area of applicability</h2>

<h3>Description</h3>

<p>This function calculates the "area of applicability" of a model, as
introduced by Meyer and Pebesma (2021). While the initial paper introducing
this method focused on spatial models, there is nothing inherently spatial
about the method; it can be used with any type of data (and, because it does
not care about the spatial arrangement of your data, can be used with 2D or
3D spatial data, and with geographic or projected CRS).
</p>


<h3>Usage</h3>

<pre><code class="language-R">ww_area_of_applicability(x, ...)

## S3 method for class 'data.frame'
ww_area_of_applicability(x, testing = NULL, importance, ..., na_rm = FALSE)

## S3 method for class 'matrix'
ww_area_of_applicability(x, testing = NULL, importance, ..., na_rm = FALSE)

## S3 method for class 'formula'
ww_area_of_applicability(
  x,
  data,
  testing = NULL,
  importance,
  ...,
  na_rm = FALSE
)

## S3 method for class 'recipe'
ww_area_of_applicability(
  x,
  data,
  testing = NULL,
  importance,
  ...,
  na_rm = FALSE
)

## S3 method for class 'rset'
ww_area_of_applicability(x, y = NULL, importance, ..., na_rm = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>Either a data frame, matrix, formula
(specifying predictor terms on the right-hand side), recipe
(from <code>recipes::recipe()</code>, or <code>rset</code> object, produced by resampling functions
from rsample or spatialsample.
</p>
<p>If <code>x</code> is a recipe, it should be the same one used to pre-process the data
used in your model. If the recipe used to build the area of applicability
doesn't match the one used to build the model, the returned area of
applicability won't be correct.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Not currently used.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>testing</code></td>
<td>
<p>A data frame or matrix containing the data used to
validate your model. This should be the same data as used to calculate all
model accuracy metrics.
</p>
<p>If this argument is <code>NULL</code>, then this function will use the training data
(from <code>x</code> or <code>data</code>) to calculate within-sample distances.
This may result in the area of applicability threshold being set too high,
with the result that too many points are classed as "inside" the area of
applicability.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>importance</code></td>
<td>
<p>Either:
</p>

<ul>
<li>
<p> A data.frame with two columns: <code>term</code>, containing the names of each
variable in the training and testing data, and <code>estimate</code>, containing
the (raw or scaled) feature importance for each variable.
</p>
</li>
<li>
<p> An object of class <code>vi</code> with at least two columns, <code>Variable</code> and
<code>Importance</code>.
</p>
</li>
</ul>
<p>All variables in the training data (<code>x</code> or <code>data</code>, depending on the context)
must have a matching importance estimate, and all terms with importance
estimates must be in the training data.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>na_rm</code></td>
<td>
<p>A logical of length 1, indicating whether observations (in both
training and testing) with <code>NA</code> values in predictors should be removed. Only
predictor variables are considered, and this value has no impact on
predictions (where <code>NA</code> values produce <code>NA</code> predictions). If <code>na_rm = FALSE</code>
and <code>NA</code> values are found, this function returns an error.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>
<p>The data frame representing your "training" data, when using the
formula or recipe methods.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>Optional: a recipe (from <code>recipes::recipe()</code>) or formula.
</p>
<p>If <code>y</code> is a recipe, it should be the same one used to pre-process the data
used in your model. If the recipe used to build the area of applicability
doesn't match the one used to build the model, the returned area of
applicability won't be correct.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Predictions made on points "inside" the area of applicability should be as
accurate as predictions made on the data provided to <code>testing</code>.
That means that generally <code>testing</code> should be your final hold-out
set so that predictions on points inside the area of applicability are
accurately described by your reported model metrics.
When passing an <code>rset</code> object to <code>x</code>, predictions made on points "inside" the
area of applicability instead should be as accurate as predictions made on
the assessment sets during cross-validation.
</p>
<p>This method assumes your model was fit using dummy variables in the place of
any non-numeric predictor, and that you have one importance score per
dummy variable. Having non-numeric predictors will cause this function to
fail.
</p>


<h3>Value</h3>

<p>A <code>ww_area_of_applicability</code> object, which can be used with <code>predict()</code> to
calculate the distance of new data to the original training data, and
determine if new data is within a model's area of applicability.
</p>


<h3>Differences from CAST</h3>

<p>This implementation differs from
Meyer and Pebesma (2021) (and therefore from CAST) when using cross-validated
data in order to minimize data leakage. Namely, in order to calculate
the dissimilarity index <code class="reqn">DI_{k}</code>, CAST:
</p>

<ol>
<li>
<p> Rescales all data used for cross validation at once, lumping assessment
folds in with analysis data.
</p>
</li>
<li>
<p> Calculates a single <code class="reqn">\bar{d}</code> as the mean distance between all points
in the rescaled data set, including between points in the same assessment
fold.
</p>
</li>
<li>
<p> For each point <code class="reqn">k</code> that's used in an assessment fold, calculates
<code class="reqn">d_{k}</code> as the minimum distance between <code class="reqn">k</code> and any point in its
corresponding analysis fold.
</p>
</li>
<li>
<p> Calculates <code class="reqn">DI_{k}</code> by dividing <code class="reqn">d_{k}</code> by <code class="reqn">\bar{d}</code> (which
was partially calculated as the distance between <code class="reqn">k</code> and the rest of
the rescaled data).
</p>
</li>
</ol>
<p>Because assessment data is used to calculate constants for rescaling analysis
data and <code class="reqn">\bar{d}</code>, the assessment data may appear too "similar" to
the analysis data when calculating <code class="reqn">DI_{k}</code>. As such, waywiser treats
each fold in an <code>rset</code> independently:
</p>

<ol>
<li>
<p> Each analysis set is rescaled independently.
</p>
</li>
<li>
<p> Separate <code class="reqn">\bar{d}</code> are calculated for each fold, as the mean distance
between all points in the analysis set for that fold.
</p>
</li>
<li>
<p> Identically to CAST, <code class="reqn">d_{k}</code> is the minimum distance between a point
<code class="reqn">k</code> in the assessment fold and any point in the
corresponding analysis fold.
</p>
</li>
<li> <p><code class="reqn">DI_{k}</code> is then found by dividing <code class="reqn">d_{k}</code> by <code class="reqn">\bar{d}</code>,
which was calculated independently from <code class="reqn">k</code>.
</p>
</li>
</ol>
<p>Predictions are made using the full training data set, rescaled once (in
the same way as CAST), and the mean <code class="reqn">\bar{d}</code> across folds, under the
assumption that the "final" model in use will be retrained using the entire
data set.
</p>
<p>In practice, this means waywiser produces very slightly higher <code class="reqn">\bar{d}</code>
values than CAST and a slightly higher area of applicability threshold than
CAST when using <code>rset</code> objects.
</p>


<h3>References</h3>

<p>H. Meyer and E. Pebesma. 2021. "Predicting into unknown space? Estimating
the area of applicability of spatial prediction models," Methods in Ecology
and Evolution 12(9), pp 1620 - 1633, doi: 10.1111/2041-210X.13650.
</p>


<h3>See Also</h3>

<p>Other area of applicability functions: 
<code>predict.ww_area_of_applicability()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">
train &lt;- vip::gen_friedman(1000, seed = 101) # ?vip::gen_friedman
test &lt;- train[701:1000, ]
train &lt;- train[1:700, ]
pp &lt;- stats::ppr(y ~ ., data = train, nterms = 11)
metric_name &lt;- ifelse(
  packageVersion("vip") &gt; package_version("0.3.2"),
  "rsq",
  "rsquared"
)

importance &lt;- vip::vi_permute(
  pp,
  target = "y",
  metric = metric_name,
  pred_wrapper = predict,
  train = train
)

aoa &lt;- ww_area_of_applicability(y ~ ., train, test, importance = importance)
predict(aoa, test)

# Equivalent methods for calculating AOA:
ww_area_of_applicability(train[2:11], test[2:11], importance)
ww_area_of_applicability(
  as.matrix(train[2:11]),
  as.matrix(test[2:11]),
  importance
)

</code></pre>


</div>