<div class="container">

<table style="width: 100%;"><tr>
<td>word2vec</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Train a word2vec model on text</h2>

<h3>Description</h3>

<p>Construct a word2vec model on text. The algorithm is explained at <a href="https://arxiv.org/pdf/1310.4546.pdf">https://arxiv.org/pdf/1310.4546.pdf</a>
</p>


<h3>Usage</h3>

<pre><code class="language-R">word2vec(
  x,
  type = c("cbow", "skip-gram"),
  dim = 50,
  window = ifelse(type == "cbow", 5L, 10L),
  iter = 5L,
  lr = 0.05,
  hs = FALSE,
  negative = 5L,
  sample = 0.001,
  min_count = 5L,
  stopwords = character(),
  threads = 1L,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>a character vector with text or the path to the file on disk containing training data or a list of tokens. See the examples.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>type</code></td>
<td>
<p>the type of algorithm to use, either 'cbow' or 'skip-gram'. Defaults to 'cbow'</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dim</code></td>
<td>
<p>dimension of the word vectors. Defaults to 50.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>window</code></td>
<td>
<p>skip length between words. Defaults to 5.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>iter</code></td>
<td>
<p>number of training iterations. Defaults to 5.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lr</code></td>
<td>
<p>initial learning rate also known as alpha. Defaults to 0.05</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>hs</code></td>
<td>
<p>logical indicating to use hierarchical softmax instead of negative sampling. Defaults to FALSE indicating to do negative sampling.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>negative</code></td>
<td>
<p>integer with the number of negative samples. Only used in case hs is set to FALSE</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sample</code></td>
<td>
<p>threshold for occurrence of words. Defaults to 0.001</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>min_count</code></td>
<td>
<p>integer indicating the number of time a word should occur to be considered as part of the training vocabulary. Defaults to 5.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>stopwords</code></td>
<td>
<p>a character vector of stopwords to exclude from training</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>threads</code></td>
<td>
<p>number of CPU threads to use. Defaults to 1.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>further arguments passed on to the methods <code>word2vec.character</code>, <code>word2vec.list</code> as well as the C++ function <code>w2v_train</code> - for expert use only</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Some advice on the optimal set of parameters to use for training as defined by Mikolov et al.
</p>

<ul>
<li>
<p>argument type: skip-gram (slower, better for infrequent words) vs cbow (fast)
</p>
</li>
<li>
<p>argument hs: the training algorithm: hierarchical softmax (better for infrequent words) vs negative sampling (better for frequent words, better with low dimensional vectors)
</p>
</li>
<li>
<p>argument dim: dimensionality of the word vectors: usually more is better, but not always
</p>
</li>
<li>
<p>argument window: for skip-gram usually around 10, for cbow around 5
</p>
</li>
<li>
<p>argument sample: sub-sampling of frequent words: can improve both accuracy and speed for large data sets (useful values are in range 0.001 to 0.00001)
</p>
</li>
</ul>
<h3>Value</h3>

<p>an object of class <code>w2v_trained</code> which is a list with elements 
</p>

<ul>
<li>
<p>model: a Rcpp pointer to the model
</p>
</li>
<li>
<p>data: a list with elements file: the training data used, stopwords: the character vector of stopwords, n
</p>
</li>
<li>
<p>vocabulary: the number of words in the vocabulary
</p>
</li>
<li>
<p>success: logical indicating if training succeeded
</p>
</li>
<li>
<p>error_log: the error log in case training failed
</p>
</li>
<li>
<p>control: as list of the training arguments used, namely min_count, dim, window, iter, lr, skipgram, hs, negative, sample, split_words, split_sents, expTableSize and expValueMax
</p>
</li>
</ul>
<h3>Note</h3>

<p>Some notes on the tokenisation
</p>

<ul>
<li>
<p>If you provide to <code>x</code> a list, each list element should correspond to a sentence (or what you consider as a sentence) and should contain a character vector of tokens. The word2vec model is then executed using <code>word2vec.list</code>
</p>
</li>
<li>
<p>If you provide to <code>x</code> a character vector or the path to the file on disk, the tokenisation into words depends on the first element provided in <code>split</code> and the tokenisation into sentences depends on the second element provided in <code>split</code> when passed on to <code>word2vec.character</code>
</p>
</li>
</ul>
<h3>References</h3>

<p><a href="https://github.com/maxoodf/word2vec">https://github.com/maxoodf/word2vec</a>, <a href="https://arxiv.org/pdf/1310.4546.pdf">https://arxiv.org/pdf/1310.4546.pdf</a>
</p>


<h3>See Also</h3>

<p><code>predict.word2vec</code>, <code>as.matrix.word2vec</code>, <code>word2vec</code>, <code>word2vec.character</code>, <code>word2vec.list</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">
library(udpipe)
## Take data and standardise it a bit
data(brussels_reviews, package = "udpipe")
x &lt;- subset(brussels_reviews, language == "nl")
x &lt;- tolower(x$feedback)

## Build the model get word embeddings and nearest neighbours
model &lt;- word2vec(x = x, dim = 15, iter = 20)
emb   &lt;- as.matrix(model)
head(emb)
emb   &lt;- predict(model, c("bus", "toilet", "unknownword"), type = "embedding")
emb
nn    &lt;- predict(model, c("bus", "toilet"), type = "nearest", top_n = 5)
nn

## Get vocabulary
vocab   &lt;- summary(model, type = "vocabulary")

# Do some calculations with the vectors and find similar terms to these
emb     &lt;- as.matrix(model)
vector  &lt;- emb["buurt", ] - emb["rustige", ] + emb["restaurants", ]
predict(model, vector, type = "nearest", top_n = 10)

vector  &lt;- emb["gastvrouw", ] - emb["gastvrij", ]
predict(model, vector, type = "nearest", top_n = 5)

vectors &lt;- emb[c("gastheer", "gastvrouw"), ]
vectors &lt;- rbind(vectors, avg = colMeans(vectors))
predict(model, vectors, type = "nearest", top_n = 10)

## Save the model to hard disk
path &lt;- "mymodel.bin"

write.word2vec(model, file = path)
model &lt;- read.word2vec(path)


## 
## Example of word2vec with a list of tokens 
## 
toks  &lt;- strsplit(x, split = "[[:space:][:punct:]]+")
model &lt;- word2vec(x = toks, dim = 15, iter = 20)
emb   &lt;- as.matrix(model)
emb   &lt;- predict(model, c("bus", "toilet", "unknownword"), type = "embedding")
emb
nn    &lt;- predict(model, c("bus", "toilet"), type = "nearest", top_n = 5)
nn

## 
## Example getting word embeddings 
##   which are different depending on the parts of speech tag
## Look to the help of the udpipe R package 
##   to get parts of speech tags on text
## 
library(udpipe)
data(brussels_reviews_anno, package = "udpipe")
x &lt;- subset(brussels_reviews_anno, language == "fr")
x &lt;- subset(x, grepl(xpos, pattern = paste(LETTERS, collapse = "|")))
x$text &lt;- sprintf("%s/%s", x$lemma, x$xpos)
x &lt;- subset(x, !is.na(lemma))
x &lt;- split(x$text, list(x$doc_id, x$sentence_id))

model &lt;- word2vec(x = x, dim = 15, iter = 20)
emb   &lt;- as.matrix(model)
nn    &lt;- predict(model, c("cuisine/NN", "rencontrer/VB"), type = "nearest")
nn
nn    &lt;- predict(model, c("accueillir/VBN", "accueillir/VBG"), type = "nearest")
nn


</code></pre>


</div>