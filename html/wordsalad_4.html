<div class="container">

<table style="width: 100%;"><tr>
<td>word2vec</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Extract word vectors from word2vec word embedding</h2>

<h3>Description</h3>

<p>The calculations are done with the word2vec package.
</p>


<h3>Usage</h3>

<pre><code class="language-R">word2vec(
  text,
  tokenizer = text2vec::space_tokenizer,
  dim = 50,
  type = c("cbow", "skip-gram"),
  window = 5L,
  min_count = 5L,
  loss = c("ns", "hs"),
  negative = 5L,
  n_iter = 5L,
  lr = 0.05,
  sample = 0.001,
  stopwords = character(),
  threads = 1L,
  collapse_character = "\t",
  composition = c("tibble", "data.frame", "matrix")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>text</code></td>
<td>
<p>Character string.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tokenizer</code></td>
<td>
<p>Function, function to perform tokenization. Defaults to
text2vec::space_tokenizer.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dim</code></td>
<td>
<p>dimension of the word vectors. Defaults to 50.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>type</code></td>
<td>
<p>the type of algorithm to use, either 'cbow' or 'skip-gram'. Defaults to 'cbow'</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>window</code></td>
<td>
<p>skip length between words. Defaults to 5.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>min_count</code></td>
<td>
<p>integer indicating the number of time a word should occur to be considered as part of the training vocabulary. Defaults to 5.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>loss</code></td>
<td>
<p>Charcter, choice of loss function must be one of "ns" or "hs".
See detaulsfor more Defaults to "ns".</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>negative</code></td>
<td>
<p>integer with the number of negative samples. Only used in case hs is set to FALSE</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n_iter</code></td>
<td>
<p>Integer, number of training iterations. Defaults to 5.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lr</code></td>
<td>
<p>initial learning rate also known as alpha. Defaults to 0.05</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sample</code></td>
<td>
<p>threshold for occurrence of words. Defaults to 0.001</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>stopwords</code></td>
<td>
<p>a character vector of stopwords to exclude from training</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>threads</code></td>
<td>
<p>number of CPU threads to use. Defaults to 1.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>collapse_character</code></td>
<td>
<p>Character vector with length 1. Character used to
glue together tokens after tokenizing. See details for more information.
Defaults to <code>"\t"</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>composition</code></td>
<td>
<p>Character, Either "tibble", "matrix", or "data.frame" for
the format out the resulting word vectors.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>A trade-off have been made to allow for an arbitrary tokenizing function. The
text is first passed through the tokenizer. Then it is being collapsed back
together into strings using <code>collapse_character</code> as the separator. You
need to pick <code>collapse_character</code> to be a character that will not appear
in any of the tokens after tokenizing is done. The default value is a "tab"
character. If you pick a character that is present in the tokens then those
words will be split.
</p>
<p>The choice of loss functions are one of:
</p>

<ul>
<li>
<p> "ns" negative sampling
</p>
</li>
<li>
<p> "hs" hierarchical softmax
</p>
</li>
</ul>
<h3>Value</h3>

<p>A tibble, data.frame or matrix containing
the token in the first column and word vectors in the remaining columns.
</p>


<h3>Source</h3>

<p><a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf</a>
</p>


<h3>References</h3>

<p>Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado,
Greg S and Dean, Jeff. 2013. Distributed Representations of Words and
Phrases and their Compositionality
</p>


<h3>Examples</h3>

<pre><code class="language-R">word2vec(fairy_tales)

# Custom tokenizer that splits on non-alphanumeric characters
word2vec(fairy_tales, tokenizer = function(x) strsplit(x, "[^[:alnum:]]+"))
</code></pre>


</div>