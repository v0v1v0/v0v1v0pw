<div class="container">

<table style="width: 100%;"><tr>
<td>dsm.projection</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Reduce Dimensionality of DSM by Subspace Projection (wordspace)
</h2>

<h3>Description</h3>

<p>Reduce dimensionality of DSM by linear projection of row vectors
into a lower-dimensional subspace.  Various projections methods with
different properties are available.
</p>


<h3>Usage</h3>

<pre><code class="language-R">
dsm.projection(model, n,
               method = c("svd", "rsvd", "asvd", "ri", "ri+svd"),
               oversampling = NA, q = 2, rate = .01, power=1,
               with.basis = FALSE, verbose = FALSE)

</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>model</code></td>
<td>

<p>either an object of class <code>dsm</code>, or a dense or sparse numeric matrix
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>

<p>projection method to use for dimensionality reduction (see “DETAILS” below)
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n</code></td>
<td>

<p>an integer specifying the number of target dimensions.  Use <code>n=NA</code> to generate as many latent dimensions as possible (i.e. the minimum of the number of rows and columns of the DSM matrix).
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>oversampling</code></td>
<td>

<p>oversampling factor for stochastic dimensionality reduction algorithms (<code>rsvd</code>, <code>asvd</code>, <code>ri+svd</code>).  If unspecified, the default value is 2 for <code>rsvd</code>, 10 for <code>asvd</code> and 10 for <code>ri+svd</code> (subject to change).
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>q</code></td>
<td>

<p>number of power iterations in the randomized SVD algorithm (Halko <em>et al.</em> 2009 recommend <code>q=1</code> or <code>q=2</code>)
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>rate</code></td>
<td>

<p>fill rate of random projection vectors.  Each random dimension has on average <code>rate * ncol(model)</code> nonzero components in the original space
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>power</code></td>
<td>
<p>apply power scaling after SVD-based projection, i.e. multiply each latent dimension with a suitable power of the corresponding singular value.
The default <code>power=1</code> corresponds to a regular orthogonal projection.  For power <code class="reqn">&gt; 1</code>, the first SVD dimensions – i.e. those capturing the main patterns of <code class="reqn">M</code> – are given more weight; for power <code class="reqn">&lt; 1</code>, they are given less weight.  The setting <code>power=0</code> results in a full equalization of the dimensions and is also known as “whitening” in the PCA case.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>with.basis</code></td>
<td>

<p>if <code>TRUE</code>, also returns orthogonal basis of the subspace as attribute of the reduced matrix (not available for random indexing methods)
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>

<p>if <code>TRUE</code>, some methods display progress messages during execution
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The following dimensionality reduction algorithms can be selected with the <code>method</code> argument:
</p>

<dl>
<dt>svd</dt>
<dd>
<p>singular value decomposition (SVD), using the efficient SVDLIBC algorithm (Berry 1992) from package <b>sparsesvd</b> if the input is a sparse matrix.  If the DSM has been scored with <code>scale="center"</code>, this method is equivalent to principal component analysis (PCA).</p>
</dd>
<dt>rsvd</dt>
<dd>
<p>randomized SVD (Halko <em>et al.</em> 2009, p. 9) based on a factorization of rank <code>oversampling * n</code> with <code>q</code> power iterations.</p>
</dd>
<dt>asvd</dt>
<dd>
<p>approximate SVD, which determines latent dimensions from a random sample of matrix rows including  <code>oversampling * n</code> data points.  This heuristic algorithm is highly inaccurate and has been <b>deprecated</b>.</p>
</dd>
<dt>ri</dt>
<dd>
<p>random indexing (RI), i.e. a projection onto random basis vectors that are approximately orthogonal. Basis vectors are generated by setting a proportion of <code>rate</code> elements randomly to <code class="reqn">+1</code> or <code class="reqn">-1</code>. Note that this does not correspond to a proper orthogonal projection, so the resulting coordinates in the reduced space should be used with caution.</p>
</dd>
<dt>ri+svd</dt>
<dd>
<p>RI to <code>oversampling * n</code> dimensions, followed by SVD of the pre-reduced matrix to the final <code>n</code> dimensions. This is not a proper orthogonal projection because the RI basis vectors in the first step are only approximately orthogonal.</p>
</dd>
</dl>
<h3>Value</h3>

<p>A numeric matrix with <code>n</code> columns (latent dimensions) and the same number of rows as the original DSM.  Some SVD-based algorithms may discard poorly conditioned singular values, returning fewer than <code>n</code> columns.
</p>
<p>If <code>with.basis=TRUE</code> and an orthogonal projection is used, the corresponding orthogonal basis <code class="reqn">B</code> of the latent subspace is returned as an attribute <code>"basis"</code>.  <code class="reqn">B</code> is column-orthogonal, hence <code class="reqn">B^T</code> projects into latent coordinates and <code class="reqn">B B^T</code> is an orthogonal subspace projection in the original coordinate system.
</p>
<p>For orthogonal projections, the attribute <code>"R2"</code> contains a numeric vector specifying the proportion of the squared Frobenius norm of the original matrix captured by each of the latent dimensions.  If the original matrix has been centered (so that a SVD projection is equivalent to PCA), this corresponds to the proportion of variance “explained” by each dimension.
</p>
<p>For SVD-based projections, the attribute <code>"sigma"</code> contains the singular values corresponding to latent dimensions.  It can be used to adjust the power scaling exponent at a later time.
</p>


<h3>Author(s)</h3>

<p>Stephanie Evert (<a href="https://purl.org/stephanie.evert">https://purl.org/stephanie.evert</a>)</p>


<h3>References</h3>

<p>Berry, Michael~W. (1992). Large scale sparse singular value computations.
<em>International Journal of Supercomputer Applications</em>, <b>6</b>, 13–49.  
</p>
<p>Halko, N., Martinsson, P. G., and Tropp, J. A. (2009).
Finding structure with randomness: Stochastic algorithms for constructing approximate matrix decompositions. Technical Report 2009-05, ACM, California Institute of Technology.
</p>


<h3>See Also</h3>

<p><code>rsvd</code> for the implementation of randomized SVD, and <code>sparsesvd</code> for the SVDLIBC wrapper
</p>


<h3>Examples</h3>

<pre><code class="language-R">
# 240 English nouns in space with correlated dimensions "own", "buy" and "sell"
M &lt;- DSM_GoodsMatrix[, 1:3]

# SVD projection into 2 latent dimensions
S &lt;- dsm.projection(M, 2, with.basis=TRUE)
  
100 * attr(S, "R2") # dim 1 captures 86.4% of distances
round(attr(S, "basis"), 3) # dim 1 = commodity, dim 2 = owning vs. buying/selling
  
S[c("time", "goods", "house"), ] # some latent coordinates
  
## Not run: 
idx &lt;- DSM_GoodsMatrix[, 4] &gt; .85 # only show nouns on "fringe"
plot(S[idx, ], pch=20, col="red", xlab="commodity", ylab="own vs. buy/sell")
text(S[idx, ], rownames(S)[idx], pos=3)

## End(Not run)
</code></pre>


</div>