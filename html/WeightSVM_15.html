<div class="container">

<table style="width: 100%;"><tr>
<td>wsvm</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Subject Weighted Support Vector Machines</h2>

<h3>Description</h3>

<p><code>wsvm</code> is used to train a subject weighted support vector machine. It can be used to carry out general regression and classification (of nu and epsilon-type), as
well as density-estimation. A formula interface is provided.
</p>


<h3>Usage</h3>

<pre><code class="language-R">## S3 method for class 'formula'
wsvm(formula, weight, data = NULL, ..., subset, na.action =
na.omit, scale = TRUE)
## Default S3 method:
wsvm(x, y = NULL, weight, scale = TRUE, type = NULL, kernel =
"radial", degree = 3, gamma = if (is.vector(x)) 1 else 1 / ncol(x),
coef0 = 0, cost = 1, nu = 0.5,
class.weights = NULL, cachesize = 100, tolerance = 0.001, epsilon = 0.1,
shrinking = TRUE, cross = 0, probability = FALSE, fitted = TRUE,
..., subset, na.action = na.omit)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>formula</code></td>
<td>
<p>a symbolic description of the model to be fit.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>
<p>an optional data frame containing the variables in the model.
By default the variables are taken from the environment which
‘wsvm’ is called from.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>a data matrix, a vector, or a sparse '<em>design</em> matrix' (object of class
<code>Matrix</code> provided by the <span class="pkg">Matrix</span> package,
or of class <code>matrix.csr</code>
provided by the <span class="pkg">SparseM</span> package, or of class
<code>simple_triplet_matrix</code> provided by the <span class="pkg">slam</span>
package). Or a kernel matrix of class <code>kernelMatrix</code> by the <span class="pkg">kernlab</span> package.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>a response vector with one label for each row/component of
<code>x</code>. Can be either a factor (for classification tasks)
or a numeric vector (for regression).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weight</code></td>
<td>
<p>the weight of each subject. It should be in the same length of <code>y</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>scale</code></td>
<td>
<p>A logical vector indicating the variables to be
scaled. If <code>scale</code> is of length 1, the value is recycled as
many times as needed.
By default, data are scaled internally (both <code>x</code> and <code>y</code>
variables) to zero mean and unit variance. The center and scale
values are returned and used for later predictions. <em>If x is a design matrix which contains dummy variables, please make these variable NOT scaled.</em></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>type</code></td>
<td>
<p><code>wsvm</code> can be used as a classification
machine, as a regression machine, or for novelty detection.
Depending of whether <code>y</code> is
a factor or not, the default setting for <code>type</code> is <code>C-classification</code> or <code>eps-regression</code>, respectively, but may be overwritten by setting an explicit value.<br>
Valid options are:
</p>

<ul>
<li> <p><code>C-classification</code>
</p>
</li>
<li> <p><code>nu-classification</code>
</p>
</li>
<li> <p><code>one-classification</code> (for novelty detection)
</p>
</li>
<li> <p><code>eps-regression</code>
</p>
</li>
<li> <p><code>nu-regression</code>
</p>
</li>
</ul>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>kernel</code></td>
<td>
<p>the kernel used in training and predicting. You
might consider changing some of the following parameters, depending
on the kernel type.<br></p>

<dl>
<dt>linear:</dt>
<dd>
<p><code class="reqn">u'v</code></p>
</dd>
<dt>polynomial:</dt>
<dd>
<p><code class="reqn">(\gamma u'v + coef0)^{degree}</code></p>
</dd>
<dt>radial basis:</dt>
<dd>
<p><code class="reqn">e^(-\gamma |u-v|^2)</code></p>
</dd>
<dt>sigmoid:</dt>
<dd>
<p><code class="reqn">tanh(\gamma u'v + coef0)</code></p>
</dd>
<dt>precomputed:</dt>
<dd>
<p>x is a precomputed kernel matrix that contains NO missing values. <code>scale</code> will not work. Cannot use <code>subset</code> and <code>na.action</code> with this kernel. </p>
</dd>
</dl>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>degree</code></td>
<td>
<p>parameter needed for kernel of type <code>polynomial</code> (default: 3)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gamma</code></td>
<td>
<p>parameter needed for all kernels except <code>linear</code>
(default: 1/(data dimension))</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>coef0</code></td>
<td>
<p>parameter needed for kernels of type <code>polynomial</code>
and <code>sigmoid</code> (default: 0)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cost</code></td>
<td>
<p>cost of constraints violation (default: 1)—it is the
‘C’-constant of the regularization term in the Lagrange formulation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nu</code></td>
<td>
<p>parameter needed for <code>nu-classification</code>,
<code>nu-regression</code>, and <code>one-classification</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>class.weights</code></td>
<td>
<p>a named vector of weights for the different
classes, used for asymmetric class sizes. Not all factor levels have
to be supplied (default weight: 1). All components have to be
named. Specifying <code>"inverse"</code> will choose the weights <em>inversely</em>
proportional to the class distribution.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cachesize</code></td>
<td>
<p>cache memory in MB (default 100)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tolerance</code></td>
<td>
<p>tolerance of termination criterion (default: 0.001)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>epsilon</code></td>
<td>
<p>epsilon in the insensitive-loss function (default: 0.1)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>shrinking</code></td>
<td>
<p>option whether to use the shrinking-heuristics
(default: <code>TRUE</code>)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cross</code></td>
<td>
<p>if a integer value k&gt;0 is specified, a k-fold cross
validation on the training data is performed to assess the quality
of the model: the accuracy rate for classification and the Mean
Squared Error for regression. Note the result is not weighted. For weighted results,
use <code>tune_wsvm</code> fucntion.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fitted</code></td>
<td>
<p>logical indicating whether the fitted values should be computed
and included in the model or not (default: <code>TRUE</code>)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>probability</code></td>
<td>
<p>logical indicating whether the model should
allow for probability predictions.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>additional parameters for the low level fitting function
<code>wsvm.default</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>subset</code></td>
<td>
<p>An index vector specifying the cases to be used in the
training sample.  (NOTE: If given, this argument must be
named.)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>na.action</code></td>
<td>
<p>A function to specify the action to be taken if <code>NA</code>s are
found. The default action is <code>na.omit</code>, which leads to rejection of cases
with missing values on any required variable. An alternative
is <code>na.fail</code>, which causes an error if <code>NA</code> cases
are found. (NOTE: If given, this argument must be named.)</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The original <code>libsvm</code> does not support subject/instance weighted svm. From the 'LIBSVM Tools' <a href="https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/#weights_for_data_instances">https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/#weights_for_data_instances</a>, we are able to use a modified version of <code>libsvm</code> to support subject weights.
</p>
<p>For multiclass-classification with k levels, k&gt;2, <code>libsvm</code> uses the
‘one-against-one’-approach, in which k(k-1)/2 binary classifiers are
trained; the appropriate class is found by a voting scheme.
</p>
<p><code>libsvm</code> internally uses a sparse data representation, which is
also high-level supported by the package <span class="pkg">SparseM</span>.
</p>
<p>If the predictor variables include factors, the formula interface must be used to get a
correct model matrix or make x a design matrix.
</p>
<p>When using the formula interface and <code>na.action</code> is <code>na.omit</code>, we delete any subjects with missing values on x, y (if exists) or weight in the training and predicting procedure (when <code>fitted = TRUE</code>). When using the x, y interface and <code>na.action</code> is <code>na.omit</code>, we delete any subjects with missing values on x, y (if exists) or weight in the training procedure, and retain the subjects with missing values only on weight in the predicting procedure (when <code>fitted = TRUE</code>).
</p>
<p><code>plot.wsvm</code> allows a simple graphical
visualization of classification models.
</p>
<p>The probability model for classification fits a logistic distribution
using maximum likelihood to the decision values of all binary
classifiers, and computes the a-posteriori class probabilities for the
multi-class problem using quadratic optimization. The probabilistic
regression model assumes (zero-mean) laplace-distributed errors for the
predictions, and estimates the scale parameter using maximum
likelihood.
</p>
<p>For linear kernel, the coefficients of the regression/decision hyperplane
can be extracted using the <code>coef</code> method (see examples).
</p>


<h3>Value</h3>

<p>An object of class <code>"wsvm"</code> containing the fitted model, including:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>SV</code></td>
<td>
<p>The resulting support vectors (possibly scaled).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>index</code></td>
<td>
<p>The index of the resulting support vectors in the data
matrix. Note that this index refers to the preprocessed data (after
the possible effect of <code>na.omit</code> and <code>subset</code>)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>coefs</code></td>
<td>
<p>The corresponding coefficients times the training labels.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>rho</code></td>
<td>
<p>The negative intercept.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sigma</code></td>
<td>
<p>In case of a probabilistic regression model, the scale
parameter of the hypothesized (zero-mean) laplace distribution estimated by
maximum likelihood.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>probA, probB</code></td>
<td>
<p>numeric vectors of length k(k-1)/2, k number of
classes, containing the parameters of the logistic distributions fitted to
the decision values of the binary classifiers (1 / (1 + exp(a x + b))).</p>
</td>
</tr>
</table>
<h3>Note</h3>

<p>Data are scaled internally, usually yielding better results.
</p>
<p>Parameters of SVM-models usually <em>must</em> be tuned to yield sensible results!
</p>


<h3>Author(s)</h3>

<p>David Meyer (based on C/C++-code by Chih-Chung Chang and Chih-Jen Lin)<br>
Modified by Tianchen Xu <a href="mailto:tx2155@columbia.edu">tx2155@columbia.edu</a>
</p>


<h3>References</h3>


<ul>
<li>
<p>Chang, Chih-Chung and Lin, Chih-Jen:<br><em>LIBSVM: a library for Support Vector Machines</em><br><a href="https://www.csie.ntu.edu.tw/~cjlin/libsvm/">https://www.csie.ntu.edu.tw/~cjlin/libsvm/</a>
</p>
</li>
<li>
<p> Ming-Wei Chang, Hsuan-Tien Lin, Ming-Hen Tsai, Chia-Hua Ho and Hsiang-Fu Yu<br><em>Weights for data instances</em><br><a href="https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/#weights_for_data_instances">https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/#weights_for_data_instances</a>
</p>
</li>
<li>
<p>Exact formulations of models, algorithms, etc. can be found in the
document:<br>
Chang, Chih-Chung and Lin, Chih-Jen:<br><em>LIBSVM: a library for Support Vector Machines</em><br><a href="https://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.ps.gz">https://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.ps.gz</a>
</p>
</li>
<li>
<p>More implementation details and speed benchmarks can be found on:
Rong-En Fan and Pai-Hsune Chen and Chih-Jen Lin:<br><em>Working Set Selection Using the Second Order Information for Training SVM</em><br><a href="https://www.csie.ntu.edu.tw/~cjlin/papers/quadworkset.pdf">https://www.csie.ntu.edu.tw/~cjlin/papers/quadworkset.pdf</a>
</p>
</li>
</ul>
<h3>See Also</h3>

<p><code>predict.wsvm</code>,
<code>plot.wsvm</code>,
<code>tune_wsvm</code>,
<code>matrix.csr</code> (in package <span class="pkg">SparseM</span>)
</p>


<h3>Examples</h3>

<pre><code class="language-R">## check what is loaded
dllpath &lt;- getLoadedDLLs()
getDLLRegisteredRoutines(dllpath$WeightSVM[[2]])

## load dataset
data(iris)

## classification mode
# default with factor response:
model1 &lt;- wsvm(Species ~ ., weight = rep(1,150), data = iris) # same weights
model2 &lt;- wsvm(x = iris[,1:4], y = iris[,5],
              weight = c(rep(0.08, 50),rep(1,100))) # less weights to setosa
# alternatively the traditional interface:
x &lt;- subset(iris, select = -Species)
y &lt;- iris$Species
model3 &lt;- wsvm(x, y, weight = rep(10,150)) # similar to model 1,
                               # but larger weights for all subjects

# These models provide error/warning info
try(wsvm(x, y)) # no weight
try(wsvm(x, y, weight = rep(10,100))) # wrong length
try(wsvm(x, y, weight = c(Inf, rep(1,149)))) # contains inf weight

print(model1)
summary(model1)

# test with train data
pred &lt;- predict(model1, iris[,1:4])
# (same as:)
pred &lt;- fitted(model1)

# Check accuracy:
table(pred, y) # model 1, equal weights

# compute decision values and probabilities:
pred &lt;- predict(model1, x, decision.values = TRUE)
attr(pred, "decision.values")[1:4,]

# visualize (classes by color, SV by crosses):
plot(cmdscale(dist(iris[,-5])),
     col = as.integer(iris[,5]),
     pch = c("o","+")[1:150 %in% model1$index + 1]) # model 1
plot(cmdscale(dist(iris[,-5])),
     col = as.integer(iris[,5]),
     pch = c("o","+")[1:150 %in% model2$index + 1])
  # In model 2, less support vectors are based on setosa


## try regression mode on two dimensions
# create data
x &lt;- seq(0.1, 5, by = 0.05)
y &lt;- log(x) + rnorm(x, sd = 0.2)

# estimate model and predict input values
model1 &lt;- wsvm(x, y, weight = rep(1,99))
model2 &lt;- wsvm(x, y, weight = seq(99,1,length.out = 99)) # decreasing weights
# library(kernlab)
# model3 &lt;- wsvm(kernlab::kernelMatrix(kernlab::rbfdot(sigma = 1), x), y,
#      weight = rep(1,99), kernel = 'precomputed') # try user defined kernel

# visualize
plot(x, y)
lines(x, log(x), col = 2)
points(x, fitted(model1), col = 4)
points(x, fitted(model2), col = 3) # better fit for the first few points
# points(x, fitted(model3), col = 5) # similar to model 1 with user defined kernel

## density-estimation
# create 2-dim. normal with rho=0:
X &lt;- data.frame(a = rnorm(1000), b = rnorm(1000))
attach(X)

# formula interface:
model &lt;- wsvm(~ a + b, gamma = 0.1, weight = c(seq(5000,1,length.out = 500),1:500))

# test:
newdata &lt;- data.frame(a = c(0, 4), b = c(0, 4))

# visualize:
plot(X, col = 1:1000 %in% model$index + 1, xlim = c(-5,5), ylim=c(-5,5))
points(newdata, pch = "+", col = 2, cex = 5)

## class weights:
i2 &lt;- iris
levels(i2$Species)[3] &lt;- "versicolor"
summary(i2$Species)
wts &lt;- 100 / table(i2$Species)
wts
m &lt;- wsvm(Species ~ ., data = i2, class.weights = wts, weight=rep(1,150))

## extract coefficients for linear kernel

# a. regression
x &lt;- 1:100
y &lt;- x + rnorm(100)
m &lt;- wsvm(y ~ x, scale = FALSE, kernel = "linear", weight = rep(1,100))
coef(m)
plot(y ~ x)
abline(m, col = "red")

# b. classification
# transform iris data to binary problem, and scale data
setosa &lt;- as.factor(iris$Species == "setosa")
iris2 = scale(iris[,-5])

# fit binary C-classification model
model1 &lt;- wsvm(setosa ~ Petal.Width + Petal.Length,
          data = iris2, kernel = "linear", weight = rep(1,150))
model2 &lt;- wsvm(setosa ~ Petal.Width + Petal.Length,
               data = iris2, kernel = "linear",
               weight = c(rep(0.08, 50),rep(1,100))) # less weights to setosa

# plot data and separating hyperplane
plot(Petal.Length ~ Petal.Width, data = iris2, col = setosa)
(cf &lt;- coef(model1))
abline(-cf[1]/cf[3], -cf[2]/cf[3], col = "red")
(cf2 &lt;- coef(model2))
abline(-cf2[1]/cf2[3], -cf2[2]/cf2[3], col = "red", lty = 2)

# plot margin and mark support vectors
abline(-(cf[1] + 1)/cf[3], -cf[2]/cf[3], col = "blue")
abline(-(cf[1] - 1)/cf[3], -cf[2]/cf[3], col = "blue")
points(model1$SV, pch = 5, cex = 2)
abline(-(cf2[1] + 1)/cf2[3], -cf2[2]/cf2[3], col = "blue", lty = 2)
abline(-(cf2[1] - 1)/cf2[3], -cf2[2]/cf2[3], col = "blue", lty = 2)
points(model2$SV, pch = 6, cex = 2)
</code></pre>


</div>