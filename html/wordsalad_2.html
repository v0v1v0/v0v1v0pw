<div class="container">

<table style="width: 100%;"><tr>
<td>fasttext</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Extract word vectors from fasttext word embedding</h2>

<h3>Description</h3>

<p>The calculations are done with the fastTextR package.
</p>


<h3>Usage</h3>

<pre><code class="language-R">fasttext(
  text,
  tokenizer = text2vec::space_tokenizer,
  dim = 10L,
  type = c("skip-gram", "cbow"),
  window = 5L,
  loss = "hs",
  negative = 5L,
  n_iter = 5L,
  min_count = 5L,
  threads = 1L,
  composition = c("tibble", "data.frame", "matrix"),
  verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>text</code></td>
<td>
<p>Character string.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tokenizer</code></td>
<td>
<p>Function, function to perform tokenization. Defaults to
text2vec::space_tokenizer.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dim</code></td>
<td>
<p>Integer, number of dimension of the resulting word vectors.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>type</code></td>
<td>
<p>Character, the type of algorithm to use, either 'cbow' or
'skip-gram'. Defaults to 'skip-gram'.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>window</code></td>
<td>
<p>Integer, skip length between words. Defaults to 5.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>loss</code></td>
<td>
<p>Charcter, choice of loss function must be one of "ns", "hs", or
"softmax". See details for more Defaults to "hs".</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>negative</code></td>
<td>
<p>integer with the number of negative samples. Only used when
loss = "ns".</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n_iter</code></td>
<td>
<p>Integer, number of training iterations. Defaults to 5.
<code>numeric = -1</code> defines early stopping strategy. Stop fitting
when one of two following conditions will be satisfied: (a) passed
all iterations (b) <code>cost_previous_iter / cost_current_iter - 1 &lt;
    convergence_tol</code>. Defaults to -1.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>min_count</code></td>
<td>
<p>Integer, number of times a token should appear to be
considered in the model. Defaults to 5.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>threads</code></td>
<td>
<p>number of CPU threads to use. Defaults to 1.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>composition</code></td>
<td>
<p>Character, Either "tibble", "matrix", or "data.frame" for
the format out the resulting word vectors.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>Logical, controls whether progress is reported as operations
are executed.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The choice of loss functions are one of:
</p>

<ul>
<li>
<p> "ns" negative sampling
</p>
</li>
<li>
<p> "hs" hierarchical softmax
</p>
</li>
<li>
<p> "softmax" full softmax
</p>
</li>
</ul>
<h3>Value</h3>

<p>A tibble, data.frame or matrix containing
the token in the first column and word vectors in the remaining columns.
</p>


<h3>Source</h3>

<p><a href="https://fasttext.cc/">https://fasttext.cc/</a>
</p>


<h3>References</h3>

<p>Enriching Word Vectors with Subword Information, 2016, P.
Bojanowski, E. Grave, A. Joulin, T. Mikolov.
</p>


<h3>Examples</h3>

<pre><code class="language-R">fasttext(fairy_tales, n_iter = 2)

# Custom tokenizer that splits on non-alphanumeric characters
fasttext(fairy_tales,
         n_iter = 2,
         tokenizer = function(x) strsplit(x, "[^[:alnum:]]+"))
</code></pre>


</div>