<div class="container">

<table style="width: 100%;"><tr>
<td>votingLinearPredictor</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2> Voting linear predictor </h2>

<h3>Description</h3>

<p>Predictor based on univariate regression on all or selected given features 
that pools all predictions using weights
derived from the univariate linear models.
</p>


<h3>Usage</h3>

<pre><code class="language-R">votingLinearPredictor(
         x, y, xtest = NULL, 
         classify = FALSE, 
         CVfold = 0, 
         randomSeed = 12345, 
         assocFnc = "cor", assocOptions = "use = 'p'", 
         featureWeightPowers = NULL, priorWeights = NULL, 
         weighByPrediction = 0, 
         nFeatures.hi = NULL, nFeatures.lo = NULL, 
         dropUnusedDimensions = TRUE, 
         verbose = 2, indent = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>

<p>Training features (predictive variables). Each column corresponds to a feature and each row to an
observation. 
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>

<p>The response variable. Can be a single vector or a matrix with arbitrary many columns. Number of rows
(observations) must equal to the number of rows (observations) in x.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>xtest</code></td>
<td>

<p>Optional test set data. A matrix of the same number of columns (i.e., features) as <code>x</code>. 
If test set data are not given, only the prediction on training data will be returned. 
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>classify</code></td>
<td>

<p>Should the response be treated as a categorical variable? Classification really only works with two classes.
(The function will run for multiclass problems as well, but the results will be sub-optimal.)
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>CVfold</code></td>
<td>

<p>Optional specification of cross-validation fold. If 0 (the default), no cross-validation is performed.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>randomSeed</code></td>
<td>

<p>Random seed, used for observation selection for cross-validation. If <code>NULL</code>, the random generator is
not reset.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>assocFnc</code></td>
<td>

<p>Function to measure association. Usually a measure of correlation, for example Pearson correlation or
<code>bicor</code>. 
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>assocOptions</code></td>
<td>

<p>Character string specifying the options to be passed to the association function.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>featureWeightPowers</code></td>
<td>

<p>Powers to which to raise the result of <code>assocFnc</code> to obtain weights. Can be a single number or a vector
of arbitrary length; the returned value will contain one prediction per power.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>priorWeights</code></td>
<td>

<p>Prior weights for the features. If given, must be either (1) a vector of the same length as the number of
features (columns in <code>x</code>); (2) a matrix of dimensions length(featureWeightPowers)x(number of features);
or (3) array of dimensions (number of response variables)xlength(featureWeightPowers)x(number of features).
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weighByPrediction</code></td>
<td>

<p>(Optional) power to downweigh features that are not well predicted between training and test sets. See
details. 
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nFeatures.hi</code></td>
<td>

<p>Optional restriction of the number of features to use. If given, this many features with the highest association
and lowest association (if <code>nFeatures.lo</code> is not given) will be used for prediction.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nFeatures.lo</code></td>
<td>

<p>Optional restriction of the number of lowest (i.e., most negatively) associated features to use. 
Only used if <code>nFeatures.hi</code> is also non-NULL.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dropUnusedDimensions</code></td>
<td>

<p>Logical: should unused dimensions be dropped from the result?
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>

<p>Integer controling how verbose the diagnostic messages should be. Zero means silent.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>indent</code></td>
<td>

<p>Indentation for the diagnostic messages. Zero means no indentation, each unit adds two spaces.
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The predictor calculates the association of each (selected) feature with the response and uses the
association to calculate the weight of the feature as <code>sign(association) *
(association)^featureWeightPower</code>. Optionally, this weight is multiplied by <code>priorWeights</code>. Further, a
feature prediction weight can be used to downweigh features that are not well predicted by other features
(see below).
</p>
<p>For classification, the (continuous) result of the above calculation is turned into ordinal values
essentially by rounding. 
</p>
<p>If features exhibit non-trivial correlations among themselves (such as, for example, in gene expression
data), one can attempt to down-weigh features that do not exhibit the same correlation in the test set.
This is done by using essentially the same predictor to predict _features_ from all other features in the
test data (using the training data to train the feature predictor). Because test features are known, the
prediction accuracy can be evaluated. If a feature is predicted badly (meaning the error in the test set is
much larger than the error in the cross-validation prediction in training data), 
it may mean that its quality in the
training or test data is low (for example, due to excessive noise or outliers). 
Such features can be downweighed using the argument <code>weighByPrediction</code>. The extra factor is 
min(1, (root mean square prediction error in test set)/(root mean square cross-validation prediction error in
the trainig data)^weighByPrediction), that is it is never bigger than 1.
</p>


<h3>Value</h3>

<p>A list with the following components:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>predicted</code></td>
<td>
<p>The back-substitution prediction on the training data. Normally an array of dimensions
(number of observations) x (number of response variables) x length(featureWeightPowers), but unused
are dropped unless <code>dropUnusedDimensions = FALSE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weightBase</code></td>
<td>
<p>Absolute value of the associations of each feature with each response.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>variableImportance</code></td>
<td>
<p>The weight of each feature in the prediction (including the sign).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>predictedTest</code></td>
<td>
<p>If input <code>xtest</code> is non-NULL, the predicted test response, in format analogous to <code>predicted</code> above.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>CVpredicted</code></td>
<td>
<p>If input <code>CVfold</code> is non-zero, cross-validation prediction on the training data.</p>
</td>
</tr>
</table>
<h3>Note</h3>

<p>It makes little practical sense to supply neither <code>xtest</code> nor <code>CVfold</code> since the prediction
accuracy on training data will be highly biased.
</p>


<h3>Author(s)</h3>

<p>Peter Langfelder
</p>


<h3>See Also</h3>

<p><code>bicor</code> for robust correlation that can be used as an association measure
</p>


</div>